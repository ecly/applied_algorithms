\documentclass[a5paper]{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[parfill]{parskip}% don't indent new sections
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\pagestyle{empty}


\title{Sum of Degrees Squared}
\author{Emil Lynegaard}

\begin{document}
  \maketitle
  \section{Sum of Derees Squared}
  \subsection{Implement}
  See attached code \texttt{sum\_degrees\_squared.go}.

  The algorithm is implemented using an array of size n, which keeps track of each vertex' degree.
  The sum is computing by squaring each degree and summing them up.

  \subsection{Question to discuss}
  \subsubsection{}
  A final scan over the array is inevitable due to the nature of squaring numbers.
  The difference between incrementing a small degree vs. \a big degree will have an impact on the output, meaning that we cannot compute any of the degrees before reading the last line.
  \subsubsection{}\label{sec:cache_misses}
  If we are accessing our array of degrees in linear order as we parse the input, we will have natural cache efficiency with minimal cache misses. On the other hand if we are constantly incrementing degrees of arbitrary vertices, we can end up with a total of $m*2$ cache misses.
  \subsubsection{}
  This phenomenon is explained by the I/O-model and the quantification is mentioned in~\ref{sec:cache_misses}
  \subsubsection{}
  For our problem, the most interesting factor is the size of \texttt{n}. For graphs with small n and large \texttt{m}, the majority of the time will be spent parsing the input as we will only ever have to do \texttt{n} squares in the end.

  To compare the platforms we would therefore use a size of \texttt{n} causing the total time to be atleast 2\textasciitilde seconds to eliminate noise in the form of mostly file I/O and OS noise.

  Factors that may be impactful for our experiment include:
  \begin{itemize}
      \item Size of m - increases file I/O time
      \item Size of n - increases summation time
      \item Input ordering - impacts cache efficieny
  \end{itemize}

  If we wanted to measure how the potentially different CPU's performed in terms of caching, we could run various graph instances on both platforms and count the cache misses as our metric. If we instead wanted measure performance in the form of time, we could run the algorithm with sufficiently large values for n and random m on both platforms measuring wall clock time as our metric. With the current implementation this would give us a good indication of single-core performance, as the algorithm is ran sequentially.
\end{document}
